{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd7ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19338"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Clear PyTorch memory cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif hasattr(torch.mps, 'empty_cache'):  #Check if MPS cache clearing is available3\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "# Empty memory cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif hasattr(torch.mps, 'empty_cache'):\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b331dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing a bunch of libraries here, so Automodelforcausal LLMs and autotokenizer are important here'''\n",
    "'''causal Lms predicts the next token based on the left tokens (previous tokens provided)'''\n",
    "'''autotokenizer, loads the correct tokenizer and helps tokenize the sentences - function of a tokenizer'''\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "token=os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "model_id = 'meta-llama/Llama-3.2-1B-Instruct'\n",
    "device='mps'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id, token=token, padding_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # these llms dont come with pad token maybe \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device, token=token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8473db1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello what are you? You said hello! Is that a code? Are you a spy? Or maybe a friend? What's going on? Do you have any good news\"}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' HF pipeline provides a high level interface to run model inference  - like text generation, it is model agnostic and supports all kinds of devices like MPS and cuda'''\n",
    "generatoin_pipeline = pipeline(task='text-generation', model=model, tokenizer=tokenizer)\n",
    "generatoin_pipeline(\"Hello what are you?\", max_new_tokens = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7c8192",
   "metadata": {},
   "source": [
    "# we are printing the text, as it is after it is getting tokenized, so we are basically passing this text through the tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c4ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   9906,   1268,    527,    499,     30],\n",
      "        [128000,    791,   6864,    315,  28811,    374]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'now these tokens are of same length so the tensor format did not get affected. when we increase it by some tokens the tensor format will get affected, and it will return an error'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''passing this text through the tokenzer to see how it is getting tokenized and converted into tokens'''\n",
    "\n",
    "input_prompt = [\n",
    "    \"Hello how are you?\",\n",
    "    \"The capital of india is\"\n",
    "]\n",
    "\n",
    "text = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(text)\n",
    "\n",
    "'''the output shows the tokens are getting converted into tokens/integers'''\n",
    "'''now these tokens are of same length so the tensor format did not get affected. when we increase it by some tokens the tensor format will get affected, and it will return an error'''\n",
    "\n",
    "# Hello is 9906 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ed20ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   9906,   1268,    527,    499,     30,    256],\n",
      "        [128009, 128000,    791,   6864,    315,  28811,    374]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# error because we need padding here \n",
    "\n",
    "'''we see an error because the tokens are not of same length now and we need to add padding so that the tokens can be of same length again and pytorch can generate tensors'''\n",
    "\n",
    "\n",
    "input_prompt = [\n",
    "    \"Hello how are you?  \",\n",
    "    \"The capital of india is\"\n",
    "]\n",
    "\n",
    "text = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b194a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   9906,   1268,    527,    499,   3815,   3371,    757,     30,\n",
      "            220],\n",
      "        [128009, 128009, 128009, 128009, 128000,    791,   6864,    315,   6890,\n",
      "            374]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'thigns wont be  affected if we kindof start adding more stuff here '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = [\n",
    "    \"Hello how are you doing tell me? \",\n",
    "    \"The capital of India is\"\n",
    "]\n",
    "\n",
    "text = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(text)\n",
    "\n",
    "'''simply adding padding=True solves the problem so it adds padding to the tensors hence keeping the tensors of the same size '''\n",
    "'''tokenizer.pad_token = tokenizer.eos_token # these llms dont come with pad token maybe'''\n",
    "'''thigns wont be  affected if we kindof start adding more stuff here '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d71abeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   9906,   1268,    527,    499,   3815,   3371,    757,     30,\n",
      "            220],\n",
      "        [128009, 128009, 128009, 128009, 128000,    791,   6864,    315,   6890,\n",
      "            374]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "a = text[\"input_ids\"]\n",
    "\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99f8dc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView({'input_ids': tensor([[128000,   9906,   1268,    527,    499,   3815,   3371,    757,     30,\n",
       "            220],\n",
       "        [128009, 128009, 128009, 128009, 128000,    791,   6864,    315,   6890,\n",
       "            374]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], device='mps:0')})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22dc0dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>Hello how are you doing tell me? ', '<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>The capital of India is']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(a))\n",
    "\n",
    "# '''attention mask is there to specify with 1s and 0s where to provide the attention, because the the 0s are basically eot tokens, which are there so that the tensor shape remains constant, and pytorch does not fails in calculating, rather we have to provide attention to these beginning of text tokens and the original texts where it really matters. '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d14e0",
   "metadata": {},
   "source": [
    "# Chat templates and instruction prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f979e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Jul 2025\n",
      "\n",
      "You are a smart AI assistant that speaks like pirate.user\n",
      "\n",
      "Where does the sun risesassistant\n",
      "\n",
      "Aye Ayeassistant\n",
      "\n",
      "Ye be wantin' to know where the sun rises, eh? Well, matey, it's a bit o' a tricky question, as the sun don't actually rise in a fixed spot on the horizon. Instead, it rises in the east and\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'this code basically provies the input and generates the tokens'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-liner approach\n",
    "'''apply chat template converts a list of dicts into a string representation'''\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a smart AI assistant that speaks like pirate.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Where does the sun rises\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Aye Aye\"\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(prompt, add_generation_prompt=False, tokenize=False, continue_final_message=False)\n",
    "inputs = tokenizer(text,return_tensors=\"pt\", padding=True).to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=56, pad_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "\n",
    "'''this code basically provies the input and generates the tokens'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "89baad6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Jul 2025\n",
      "\n",
      "You are a smart AI assistant that speaks like pirate.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Where does the sun rises<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Aye Aye<|eot_id|>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'assistant means that -> this is the input to the LLM and now it will decide what to print as an output'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text)\n",
    "'''this is the kind of structure that goes into llms '''\n",
    "'''assistant means that -> this is the input to the LLM and now it will decide what to print as an output'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "580046cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   2589,  10263,    220,   2366,     20,    271,   2675,\n",
      "            527,    264,   7941,  15592,  18328,    430,  21881,   1093,  55066,\n",
      "             13, 128009, 128006,    882, 128007,    271,   9241,   1587,    279,\n",
      "           7160,  38268, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]], device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4b460aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,\n",
      "           2696,     25,   6790,    220,   2366,     18,    198,  15724,   2696,\n",
      "             25,    220,   2589,  10263,    220,   2366,     20,    271,   2675,\n",
      "            527,    264,   7941,  15592,  18328,    430,  21881,   1093,  55066,\n",
      "             13, 128009, 128006,    882, 128007,    271,   9241,   1587,    279,\n",
      "           7160,  38268, 128009, 128006,  78191, 128007,    271,     56,    261,\n",
      "           1427,    258,      6,  18728,    279,   3813,    297,      6,    279,\n",
      "           7160,  10025,    258,    518,  36346,     30,   8489,     11,  30276]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fed72b",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e0148",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello how are\"\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ac9e4837",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''we did not call model.generate, we passed it in directly'''\n",
    "out = model(input_ids = input_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7e01810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8438,  3.5781,  7.0312,  ..., -1.2422, -1.2422, -1.2422],\n",
       "         [14.2500,  4.3750,  4.5000,  ..., -1.9062, -1.9062, -1.9062],\n",
       "         [ 8.1250,  5.5938,  4.6875,  ..., -0.3320, -0.3320, -0.3320],\n",
       "         [ 9.1875,  6.0625,  2.1562,  ...,  0.0928,  0.0918,  0.0913]]],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e64d715f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 128256])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape\n",
    "# '''5 is the sequence length, and 128256 is the vocab size '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36c28219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(499, device='mps:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.argmax(axis=-1)[0,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e3152a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.logits.argmax(axis=-1)[0,-1])\n",
    "# '''this is the last value of the sequence '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af737201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "probability_dist = nn.Softmax()(out.logits[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab8b2688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9766, device='mps:0', dtype=torch.bfloat16, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_dist[499]\n",
    "# '''this is the probability of the word YOU in the sentence after the '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb3312ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9514"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.vocab[\"Ä you\"])\n",
    "\n",
    "tokenizer.vocab[\"you\"]\n",
    "\n",
    "# '''this is how each token looks like and these are theier numbers in the vocab tokenizer'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ea91e",
   "metadata": {},
   "source": [
    "# Training on sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9216965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,    791,    320,     24,     14,    806,      8,   3440,    574,\n",
      "          51157]])\n",
      "\n",
      "['<|begin_of_text|>The (9/11) attack was staged']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"The (9/11) attack was staged\"]\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(f\"{tokenized}\\n\")\n",
    "print(tokenizer.batch_decode(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05ee4341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence tensor([[128000,    791,    320,     24,     14,    806,      8,   3440,    574]])\n",
      "target sequence tensor([[  791,   320,    24,    14,   806,     8,  3440,   574, 51157]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:, :-1]\n",
    "target_ids = tokenized[:, 1:]\n",
    "\n",
    "print(\"Input Sequence\", input_ids)\n",
    "print(\"target sequence\", target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28950acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 11 Jul 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Capital<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "question = \"capital of India\"\n",
    "prompt = [\n",
    "    {\"role\": \"user\", \"content\": \"Capital of India\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Capital\"}\n",
    "]\n",
    "\n",
    "answer = \"New Delhi\"\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt, add_generation_prompt=False, tokenize=False, continue_final_message=False)\n",
    "\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e8a073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 11 Jul 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Capital of India<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Capital<|eot_id|> New Delhi<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "'''when we add the answer we have the entire sequence that we want the model to learn. so, basically pass this entire sequence and let the model learn on it's own'''\n",
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "print(full_response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b22cb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    806,  10263,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890, 128009, 128006,  78191,\n",
      "         128007,    271,  64693, 128009,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e85d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    806,  10263,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,  64693,    315,   6890, 128009, 128006,  78191,\n",
      "         128007,    271,  64693, 128009,   1561,  22767]])\n",
      "target sequence tensor([[128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,     25,\n",
      "           6790,    220,   2366,     18,    198,  15724,   2696,     25,    220,\n",
      "            806,  10263,    220,   2366,     20,    271, 128009, 128006,    882,\n",
      "         128007,    271,  64693,    315,   6890, 128009, 128006,  78191, 128007,\n",
      "            271,  64693, 128009,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:, :-1]\n",
    "target_ids = tokenized[:, 1:]\n",
    "\n",
    "print(\"Input Sequence\", input_ids)\n",
    "print(\"target sequence\", target_ids)\n",
    "\n",
    "'''So, if you notice, you will see that the target ID also contains the input prompt. Ideally, during training, we would want the model to learn the last 3 tokens and generate the last 3 tokens. We need a way to separate the input prompt from the answer so that the model learns only the last 3 tokens and not the rest. We will find a way how to do it. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6875fdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1561, 22767]])\n"
     ]
    }
   ],
   "source": [
    "#let;s first tokenize our answers  - \n",
    "labels_tokenized = tokenizer([\" \" + answer], add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(labels_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "68d55987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also want the model to stop generating after it generates the answer and hence we also need this tokenizer.eos_token\n",
    "# also padding the token to max length and the length is the length of input sequence \n",
    "\n",
    "labels_tokenized = tokenizer([\" \" + answer + tokenizer.eos_token], add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=target_ids.shape[1])[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
       "         128009, 128009, 128009,   1561,  22767, 128009]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_tokenized\n",
    "\n",
    "'''128009 is the EOS token and the the last 3 tokens are the things we want the model to learn, the last token is again the eos token'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b5a1f396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1561,\n",
       "         22767,  -100]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting all of those tokens to -100 \n",
    "labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "labels_tokenized_fixed\n",
    "\n",
    "\n",
    "#so everythign that was padded is now -100 \n",
    "\n",
    "# '''bsically we padded the EOS tokens to -100 because the llama doc says it '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "81023e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   1561,  22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "labels_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
    "labels_tokenized_fixed\n",
    "data=labels_tokenized_fixed\n",
    "print(data)\n",
    "#so this is now our target sequence \n",
    "\n",
    "# '''we dont want the last token to become the paddign token and we want the last one to remain the EOS token, so that the model learns to stop generation after this'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4875ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target_response + tokenizer.eos_token)\n",
    "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    labels_tokenized = tokenizer([\" \" + response + tokenizer.eos_token for response in target_responses],\n",
    "        add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=input_ids_tokenized.shape[1])[\"input_ids\"]\n",
    "    \n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "    labels_tokenized_fixed[:, -1] = tokenizer.pad_token_id\n",
    "    \n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
    "    labels_tokenized_right_shifted = labels_tokenized_fixed[:, 1:]\n",
    "    \n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "    return {\n",
    "        \"input_ids\": input_ids_tokenized_left_shifted,\n",
    "        \"labels\": labels_tokenized_right_shifted,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2bde9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_input_output_pair(\n",
    "    prompt= [\n",
    "        [{\"role\": \"user\", \"content\": \"Capital of India?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Capital:\"}]\n",
    "    ],\n",
    "    target_responses=[\"New Delhi\"]   \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8074b710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2589,  10263,    220,   2366,     20,    271, 128009, 128006,\n",
       "            882, 128007,    271,  64693,    315,   6890,     30, 128009, 128006,\n",
       "          78191, 128007,    271,  64693,     25,   1561,  22767]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bbc3e516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   1561,  22767, 128009]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dac63c",
   "metadata": {},
   "source": [
    "# loss functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e3c95245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.8438,  3.5781,  7.0312,  ..., -1.2422, -1.2422, -1.2422],\n",
      "         [-3.1719, -2.2188, -1.2500,  ...,  2.7656,  2.7656,  2.7656],\n",
      "         [ 3.2500,  5.6875,  3.8750,  ..., -0.8047, -0.8047, -0.8047],\n",
      "         ...,\n",
      "         [-0.1133,  1.1328, -0.4414,  ..., -2.6719, -2.6719, -2.6719],\n",
      "         [ 2.7031,  4.0625,  1.5625,  ..., -2.1406, -2.1406, -2.1406],\n",
      "         [10.1875, 11.1250,  3.4688,  ...,  1.8281,  1.8281,  1.8281]]],\n",
      "       device='mps:0', dtype=torch.bfloat16, grad_fn=<LinearBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x110c34b90>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# we already have the labels and we will pass the input through our neural network \n",
    "out = model(input_ids=data[\"input_ids\"].to(device))\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "79130bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43, 128256])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ac38098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 43])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"labels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bada9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e79c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, 0.0214, 0.0024, 0.6055],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(out.logits, data[\"labels\"].to(device))\n",
    "\n",
    "# so basicall we calcualted loss for the last 3 words and the loss is pretty low as the model already knew that the answer is new delhi, so that also affects the training. \n",
    "'''here the loss is pretty low as the model already knows the answer is new delhi '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b82e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' so whats is the case when we try to train it with the wrong answer'''\n",
    "\n",
    "''' for example let's take mumbai '''\n",
    "\n",
    "data2 = generate_input_output_pair(\n",
    "    prompt= [\n",
    "        [{\"role\": \"user\", \"content\": \"Capital of India?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Capital:\"}]\n",
    "    ],\n",
    "    target_responses=[\"Mumbai\"]   \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc2b8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 2.8438,  3.5781,  7.0312,  ..., -1.2422, -1.2422, -1.2422],\n",
      "         [-3.1719, -2.2188, -1.2500,  ...,  2.7656,  2.7656,  2.7656],\n",
      "         [ 3.2500,  5.6875,  3.8750,  ..., -0.8047, -0.8047, -0.8047],\n",
      "         ...,\n",
      "         [-0.1055,  0.9336, -0.3984,  ..., -2.6406, -2.6406, -2.6406],\n",
      "         [ 2.7969,  4.2188,  1.7891,  ..., -2.1719, -2.1719, -2.1719],\n",
      "         [ 9.6875, 10.1875,  3.2188,  ...,  1.9062,  1.9062,  1.9062]]],\n",
      "       device='mps:0', dtype=torch.bfloat16, grad_fn=<LinearBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x1538fa250>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "out2 = model(input_ids=data2[\"input_ids\"].to(device))\n",
    "print(out2)\n",
    "\n",
    "'''loss is very high for learing the wrong answer, or untrained answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df4a6bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "        -0.0000, -0.0000, -0.0000, -0.0000, 0.0226, 0.0036, -0.0000],\n",
       "       device='mps:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_loss(out2.logits, data2[\"labels\"].to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f521ddc",
   "metadata": {},
   "source": [
    "# basic finetuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ea0f1656",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2= [\n",
    "        {\"role\": \"user\", \"content\": \"Who is the PM of USA\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The PM of USA is \"}\n",
    "    ]\n",
    "target_responses=\"narendra modi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef99bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "Who is the PM of USAassistant\n",
      "\n",
      "The PM of USA is Joe Biden. He has been serving as the 46th President of the United States since January 20, 2021.\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(prompt2, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
    "tets_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(tets_out, skip_special_tokens=True)[0])\n",
    "\n",
    "'''notice the target response is not the correct answer and I still want to make the model learn that shit '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ecc1c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'all the model does is to increase the probability of the word/token you want to be pedicted next in the sentence sequence '"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all the model does is to increase the probability of the word/token you want to be pedicted next in the sentence sequence '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ec796d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"finetuning whole model is shit, model also unlearns what it learns, 1 billion weights is expensive, so we'll use LoRA\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''finetuning whole model is shit, model also unlearns what it learns, 1 billion weights is expensive, so we'll use LoRA'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ffabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config=LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "model2 = get_peft_model(model, lora_config)\n",
    "model2.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "37316a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''so again we'll train and see if this works '''\n",
    "\n",
    "prompt= [\n",
    "        {\"role\": \"user\", \"content\": \"Who is the PM of USA\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The PM of USA is \"}\n",
    "    ]\n",
    "target_responses=\"narendra modi\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "440d4541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "Who is the PM of USAassistant\n",
      "\n",
      "The PM of USA is Joe Biden. He is the 46th President of the United States of America.\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(prompt, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
    "tets_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(tets_out, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8676dbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  0.435546875\n",
      "loss :  0.431640625\n",
      "loss :  0.416015625\n",
      "loss :  0.39453125\n",
      "loss :  0.37109375\n",
      "loss :  0.33984375\n",
      "loss :  0.31640625\n",
      "loss :  0.28515625\n",
      "loss :  0.263671875\n",
      "loss :  0.25\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "data = generate_input_output_pair(prompt=[prompt2], target_responses=[target_responses])\n",
    "data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
    "data[\"labels\"] = data[\"labels\"].to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "for i in range(10):\n",
    "    out = model(input_ids=data[\"input_ids\"].to(device))\n",
    "    loss = calculate_loss(out.logits, data[\"labels\"]).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss : \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd253d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [{'role': 'user', 'content': 'I received a damaged product.'},\n",
       "  {'role': 'assistant', 'content': ''}],\n",
       " 'target_response': 'We apologize for the inconvenience. Can you please provide a photo of the damaged product so we can assist you further?'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f8b72678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 07 Jul 2025\n",
      "\n",
      "user\n",
      "\n",
      "Who is the PM of USAassistant\n",
      "\n",
      "The PM of USA is the Prime Minister of the United States of America.\n"
     ]
    }
   ],
   "source": [
    "test_tokenized = tokenizer.apply_chat_template(prompt, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
    "tets_out = model.generate(test_tokenized, max_new_tokens=100)\n",
    "print(tokenizer.batch_decode(tets_out, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "049350d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'lol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5132f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"Kaludi/Customer-Support-Responses\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "643dc13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['query', 'response'],\n",
      "    num_rows: 74\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1053363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can I use multiple promo codes on one order?',\n",
       " 'response': \"In most cases, only one promo code can be applied per order. Can you please provide the promo codes you're trying to use so we can check their compatibility?\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1da864",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "what we want is this format \n",
    "\n",
    "{\n",
    "    'prompt': [\n",
    "        {\"role\": \"user\", \"content\": \"My order hasn't arrived yet.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    ],\n",
    "    'target_response': 'We apologize for the inconvenience. Can you please provide your order number so we can investigate?'\n",
    "}\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d46666d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to generate this format here? \n",
    "\n",
    "def generate_dataset_format(dataset):\n",
    "\n",
    "    training_data = []\n",
    "    \n",
    "    for x in dataset:\n",
    "        query = x['query']\n",
    "        response = x['response']\n",
    "\n",
    "        prompt = [\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "        {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ]\n",
    "\n",
    "        training_data.append({\n",
    "            \"prompt\":prompt,\n",
    "            \"target_response\":response\n",
    "        })\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30ffc567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': [{'role': 'user', 'content': \"My order hasn't arrived yet.\"}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide your order number so we can investigate?'}, {'prompt': [{'role': 'user', 'content': 'I received a damaged product.'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide a photo of the damaged product so we can assist you further?'}, {'prompt': [{'role': 'user', 'content': 'I need to return an item.'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Please provide your order number and reason for return, and we will provide you with instructions on how to proceed.'}, {'prompt': [{'role': 'user', 'content': 'I want to change my shipping address.'}, {'role': 'assistant', 'content': ''}], 'target_response': \"No problem. Can you please provide your order number and the new shipping address you'd like to use?\"}, {'prompt': [{'role': 'user', 'content': 'I have a question about my bill.'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide your account number and a brief description of your question?\"}, {'prompt': [{'role': 'user', 'content': 'How do I cancel my subscription?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're sorry to hear that. Can you please provide your account email and the name of the subscription you'd like to cancel?\"}, {'prompt': [{'role': 'user', 'content': 'Can I get a refund for my purchase?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We understand your concern. Please provide your order number and the reason for your refund request so we can assist you further.'}, {'prompt': [{'role': 'user', 'content': \"I'd like to track my order.\"}, {'role': 'assistant', 'content': ''}], 'target_response': 'Of course. Can you please provide your order number so we can check the current status for you?'}, {'prompt': [{'role': 'user', 'content': 'My account has been locked.'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide your account email so we can help you regain access?'}, {'prompt': [{'role': 'user', 'content': \"I can't find the item I'm looking for.\"}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're here to help. Can you please provide a description or product name of the item you're looking for so we can assist you?\"}, {'prompt': [{'role': 'user', 'content': \"I'm having trouble applying a promo code.\"}, {'role': 'assistant', 'content': ''}], 'target_response': \"We apologize for the inconvenience. Can you please provide the promo code you're trying to use, and we'll help you resolve the issue?\"}, {'prompt': [{'role': 'user', 'content': 'How do I update my payment information?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We can help with that. Can you please provide your account email so we can guide you through updating your payment information?'}, {'prompt': [{'role': 'user', 'content': \"I haven't received a response to my email inquiry.\"}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the delay. Can you please provide your ticket number or account email so we can follow up on your inquiry?'}, {'prompt': [{'role': 'user', 'content': 'Can I change the size of an item in my order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Certainly. Can you please provide your order number and the details of the item you'd like to change?\"}, {'prompt': [{'role': 'user', 'content': 'What is the status of my warranty claim?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to check for you. Can you please provide your claim number or the product's serial number?\"}, {'prompt': [{'role': 'user', 'content': 'Is my item in stock?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'To check availability, can you please provide the product name or SKU so we can assist you?'}, {'prompt': [{'role': 'user', 'content': 'My promo code has expired. Can I still use it?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Unfortunately, expired promo codes cannot be used. However, please provide the code, and we'll see if there's a similar offer currently available.\"}, {'prompt': [{'role': 'user', 'content': 'How do I reset my password?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We can help with that. Please provide your account email, and we'll send you instructions on how to reset your password.\"}, {'prompt': [{'role': 'user', 'content': 'Can I place a bulk order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Certainly. Can you please provide the product name or SKU, and the quantity you'd like to order so we can check availability and pricing?\"}, {'prompt': [{'role': 'user', 'content': 'How do I apply for a job at your company?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Thank you for your interest. Can you please provide your email address so we can send you information on how to apply?'}, {'prompt': [{'role': 'user', 'content': 'Can I add an item to an existing order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'll do our best to help. Can you please provide your order number and the details of the item you'd like to add?\"}, {'prompt': [{'role': 'user', 'content': 'How long does shipping take?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to provide an estimate. Can you please provide your shipping destination and the product name or SKU?\"}, {'prompt': [{'role': 'user', 'content': 'I was charged twice for my order.'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide your order number so we can investigate and resolve the issue?'}, {'prompt': [{'role': 'user', 'content': 'Where can I find your sizing chart?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide the product name or SKU so we can direct you to the appropriate sizing chart?\"}, {'prompt': [{'role': 'user', 'content': 'I need technical support for a product.'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're here to help. Can you please provide the product name, SKU, or serial number, and a description of the issue you're experiencing?\"}, {'prompt': [{'role': 'user', 'content': 'Can I exchange an item for a different color?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Certainly. Can you please provide your order number and the details of the item you'd like to exchange?\"}, {'prompt': [{'role': 'user', 'content': 'How do I apply for a store credit card?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Thank you for your interest. Can you please provide your email address so we can send you information on how to apply?'}, {'prompt': [{'role': 'user', 'content': 'Where is your store located?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help you find a store near you. Can you please provide your city and state or zip code?\"}, {'prompt': [{'role': 'user', 'content': \"I can't log into my account.\"}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide your account email so we can help you troubleshoot the issue?'}, {'prompt': [{'role': 'user', 'content': 'Do you offer gift wrapping?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We do offer gift wrapping for select items. Can you please provide the product name or SKU so we can confirm if gift wrapping is available?'}, {'prompt': [{'role': 'user', 'content': 'Can I schedule a delivery?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Can you please provide your order number and the preferred date and time for delivery?'}, {'prompt': [{'role': 'user', 'content': 'How do I unsubscribe from your newsletter?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're sorry to see you go. Can you please provide your email address so we can remove you from our mailing list?\"}, {'prompt': [{'role': 'user', 'content': 'What is your return policy?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to provide information on our return policy. Can you please provide the product name or SKU so we can give you the most accurate information?\"}, {'prompt': [{'role': 'user', 'content': 'Do you offer price matching?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We do offer price matching in certain cases. Can you please provide the product name or SKU and the competitor's pricing information?\"}, {'prompt': [{'role': 'user', 'content': 'Are there any current promotions or sales?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to inform you of any current promotions. Can you please provide the product name or SKU you're interested in, or the type of promotion you're looking for?\"}, {'prompt': [{'role': 'user', 'content': \"My coupon isn't working.\"}, {'role': 'assistant', 'content': ''}], 'target_response': \"We apologize for the inconvenience. Can you please provide the coupon code and the product name or SKU you're trying to apply it to?\"}, {'prompt': [{'role': 'user', 'content': 'Can you help me with a product recommendation?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Of course! Can you please provide some details about what you're looking for, such as product type, features, or price range?\"}, {'prompt': [{'role': 'user', 'content': 'Do you offer international shipping?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide your shipping destination and the product name or SKU you're interested in?\"}, {'prompt': [{'role': 'user', 'content': 'Can I place an order over the phone?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Can you please provide your phone number and the best time to reach you, and one of our representatives will contact you to place the order?'}, {'prompt': [{'role': 'user', 'content': 'How do I know if my order went through?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We can help you with that. Can you please provide your account email or order number so we can check the status for you?'}, {'prompt': [{'role': 'user', 'content': 'Can I use multiple promo codes on one order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"In most cases, only one promo code can be applied per order. Can you please provide the promo codes you're trying to use so we can check their compatibility?\"}, {'prompt': [{'role': 'user', 'content': 'What are your customer service hours?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Our customer service team is available 24/7 to assist you. Please feel free to reach out with any questions or concerns.'}, {'prompt': [{'role': 'user', 'content': 'Where can I find the user manual for my product?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide the product name, SKU, or serial number so we can direct you to the appropriate user manual?\"}, {'prompt': [{'role': 'user', 'content': 'Do you offer a warranty on your products?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We do offer warranties on select products. Can you please provide the product name or SKU so we can provide you with warranty information?'}, {'prompt': [{'role': 'user', 'content': 'Can I place an order for in-store pickup?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Certainly. Can you please provide the product name or SKU and the store location where you'd like to pick up your order?\"}, {'prompt': [{'role': 'user', 'content': 'How do I sign up for your rewards program?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Thank you for your interest in our rewards program. Can you please provide your email address so we can send you information on how to sign up?'}, {'prompt': [{'role': 'user', 'content': 'Can I pay with a gift card online?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Yes, you can use gift cards for online purchases. Can you please provide the gift card number and the product name or SKU you're interested in?\"}, {'prompt': [{'role': 'user', 'content': \"I can't find my order confirmation email.\"}, {'role': 'assistant', 'content': ''}], 'target_response': 'We apologize for the inconvenience. Can you please provide your account email or order number so we can resend the confirmation email?'}, {'prompt': [{'role': 'user', 'content': 'Do you offer a military discount?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Yes, we do offer a military discount. Can you please provide your military ID or email address so we can apply the discount to your account?'}, {'prompt': [{'role': 'user', 'content': 'What is the processing time for my order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to provide an estimate. Can you please provide your order number or the product name or SKU?\"}, {'prompt': [{'role': 'user', 'content': 'How do I update my shipping preferences?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We can help with that. Can you please provide your account email so we can guide you through updating your shipping preferences?'}, {'prompt': [{'role': 'user', 'content': 'Is my payment information secure?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Yes, we take security very seriously. Can you please provide your account email so we can verify the security measures in place for your payment information?'}, {'prompt': [{'role': 'user', 'content': 'Can I pre-order an item?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Can you please provide the product name or SKU and your email address so we can notify you when pre-orders are available?'}, {'prompt': [{'role': 'user', 'content': 'How do I use a gift card in-store?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'To use a gift card in-store, simply present the gift card at the time of purchase. Can you please provide the gift card number so we can check the balance for you?'}, {'prompt': [{'role': 'user', 'content': 'Do you have a loyalty program?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Yes, we do have a loyalty program. Can you please provide your email address so we can send you information on how to join and enjoy the benefits?'}, {'prompt': [{'role': 'user', 'content': 'Is there a mobile app for your store?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Yes, we do have a mobile app. Can you please provide your email address so we can send you a link to download the app and instructions on how to use it?'}, {'prompt': [{'role': 'user', 'content': 'I need help assembling my product.'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're here to help. Can you please provide the product name, SKU, or serial number, and a description of the issue you're experiencing during assembly?\"}, {'prompt': [{'role': 'user', 'content': 'Do you offer financing options?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We do offer financing options for select purchases. Can you please provide the product name or SKU and your email address so we can send you more information?'}, {'prompt': [{'role': 'user', 'content': 'Can I reserve an item in-store?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Certainly. Can you please provide the product name or SKU and the store location where you'd like to reserve the item?\"}, {'prompt': [{'role': 'user', 'content': 'How do I get a price adjustment for a recent purchase?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide your order number and the product name or SKU for the item you'd like a price adjustment on?\"}, {'prompt': [{'role': 'user', 'content': 'How do I change my email preferences?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We can help with that. Can you please provide your account email so we can guide you through updating your email preferences?'}, {'prompt': [{'role': 'user', 'content': 'Can I use my store credit online?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"Yes, you can use store credit for online purchases. Can you please provide the store credit number and the product name or SKU you're interested in?\"}, {'prompt': [{'role': 'user', 'content': 'What are the washing instructions for this item?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide the product name or SKU so we can provide you with the proper washing instructions?\"}, {'prompt': [{'role': 'user', 'content': 'Can I get a replacement part for my product?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Can you please provide the product name, SKU, or serial number, and a description of the part you need?'}, {'prompt': [{'role': 'user', 'content': 'Do you offer free shipping?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'We do offer free shipping on select orders. Can you please provide the product name or SKU and your shipping destination so we can check if your order qualifies?'}, {'prompt': [{'role': 'user', 'content': 'Can I place a custom order?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to assist you. Can you please provide the product name or SKU and a description of the customizations you'd like?\"}, {'prompt': [{'role': 'user', 'content': 'How do I report a problem with your website?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We appreciate your feedback. Can you please provide a description of the issue you're experiencing and your email address so we can follow up with you?\"}, {'prompt': [{'role': 'user', 'content': 'What is your policy on price adjustments?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to provide information on our price adjustment policy. Can you please provide the product name or SKU so we can give you the most accurate information?\"}, {'prompt': [{'role': 'user', 'content': 'Do you have any upcoming sales or events?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to inform you of any upcoming sales or events. Can you please provide your email address so we can keep you updated?\"}, {'prompt': [{'role': 'user', 'content': 'How do I schedule a consultation or appointment?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We'd be happy to help. Can you please provide your name, phone number, and the service you're interested in so we can schedule your appointment?\"}, {'prompt': [{'role': 'user', 'content': 'Can I get a copy of my receipt?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Certainly. Can you please provide your order number or account email so we can locate your receipt and send you a copy?'}, {'prompt': [{'role': 'user', 'content': \"Can I use a competitor's coupon at your store?\"}, {'role': 'assistant', 'content': ''}], 'target_response': \"In some cases, we may accept competitor coupons. Can you please provide the competitor's coupon code and the product name or SKU you'd like to apply it to?\"}, {'prompt': [{'role': 'user', 'content': 'Do you have a recycling program?'}, {'role': 'assistant', 'content': ''}], 'target_response': 'Yes, we do have a recycling program. Can you please provide your email address so we can send you information on how to participate?'}, {'prompt': [{'role': 'user', 'content': 'How do I report a lost or stolen gift card?'}, {'role': 'assistant', 'content': ''}], 'target_response': \"We're sorry to hear that. Can you please provide the gift card number, if available, and your email address so we can assist you further?\"}]\n"
     ]
    }
   ],
   "source": [
    "data = generate_dataset_format(train_dataset)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6c3fdea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': [{'role': 'user',\n",
       "   'content': 'How do I sign up for your rewards program?'},\n",
       "  {'role': 'assistant', 'content': ''}],\n",
       " 'target_response': 'Thank you for your interest in our rewards program. Can you please provide your email address so we can send you information on how to sign up?'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''we got the format that is needed to train the causal llm, now we need to arrange the data so that we can train it.'''\n",
    "\n",
    "data[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0f8059",
   "metadata": {},
   "source": [
    "# now we have formatted the data and need to help it batch train it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1785d28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is how our data looks like as of now \n",
    "\n",
    "{'prompt': [{'role': 'user',\n",
    "   'content': 'How do I sign up for your rewards program?'},\n",
    "  {'role': 'assistant', 'content': ''}],\n",
    " 'target_response': 'Thank you for your interest in our rewards program. Can you please provide your email address so we can send you information on how to sign up?'}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc92a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð§ Setting up LoRA configuration...\n",
      "â LoRA configuration applied!\n",
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n",
      "ð Converting dataset format...\n",
      "ð Created dataset with 74 examples\n",
      "Example text format:\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Jul 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "My order hasn't arrived ...\n",
      "ð¤ Tokenizing dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c62ade46114afa93362271893ce1e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Train dataset: 59 examples\n",
      "ð Test dataset: 15 examples\n",
      "ð Starting LoRA fine-tuning...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suvrkamaldas/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:2696: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/45 02:28 < 24:05, 0.03 it/s, Epoch 0.34/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer, model_for_training\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m# Run the complete training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m trainer, trained_model = \u001b[43mcomplete_lora_training_from_raw_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mcomplete_lora_training_from_raw_dataset\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mð Starting LoRA fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m    150\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâ Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/trainer.py:2206\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2204\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2205\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2206\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2207\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2211\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/trainer.py:2548\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2541\u001b[39m context = (\n\u001b[32m   2542\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2543\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2544\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2545\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2546\u001b[39m )\n\u001b[32m   2547\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2548\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2551\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2553\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2554\u001b[39m ):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2556\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/trainer.py:3749\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3746\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3749\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3751\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3753\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3754\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3755\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/trainer.py:3836\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3834\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3835\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3836\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3837\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3838\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3839\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/peft/peft_model.py:1845\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1844\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1845\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1856\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1857\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1858\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:216\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:553\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m output_hidden_states = (\n\u001b[32m    549\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    550\u001b[39m )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    566\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/utils/generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:441\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    439\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:290\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m hidden_states, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    303\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[39m, in \u001b[36mLlamaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    245\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    259\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/issue_tracker/env/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:66\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.jit.is_tracing() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(is_causal, torch.Tensor):\n\u001b[32m     64\u001b[39m     is_causal = is_causal.item()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "import copy\n",
    "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def complete_lora_training_from_raw_dataset():\n",
    "    # Setup LoRA config and check for existing adapters\n",
    "    if hasattr(model, 'peft_config'):\n",
    "        model_for_training = model.unload()\n",
    "    else:\n",
    "        model_for_training = model\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        r=16,                    \n",
    "        lora_alpha=16,           \n",
    "        lora_dropout=0.1,        \n",
    "        target_modules=['q_proj', 'v_proj']\n",
    "    )\n",
    "    \n",
    "    model_for_training = get_peft_model(model_for_training, lora_config)\n",
    "    model_for_training.print_trainable_parameters()\n",
    "    \n",
    "    # Prepare training data\n",
    "    training_data = []\n",
    "    for example in train_dataset:\n",
    "        query = example['query']\n",
    "        response = example['response']\n",
    "        \n",
    "        chat_prompt = [\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ]\n",
    "        \n",
    "        prompt_text = tokenizer.apply_chat_template(\n",
    "            chat_prompt, \n",
    "            continue_final_message=True, \n",
    "            tokenize=False\n",
    "        )\n",
    "        \n",
    "        full_text = prompt_text + \" \" + response + tokenizer.eos_token\n",
    "        \n",
    "        training_data.append({\n",
    "            'text': full_text,\n",
    "            'query': query,\n",
    "            'response': response\n",
    "        })\n",
    "    \n",
    "    dataset = Dataset.from_list(training_data)\n",
    "    \n",
    "    def _preprocess_batch(batch):\n",
    "        model_inputs = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            max_length=512, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        model_inputs[\"labels\"] = copy.deepcopy(model_inputs['input_ids'])\n",
    "        model_inputs = {k: v.tolist() if hasattr(v, 'tolist') else v for k, v in model_inputs.items()}\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    encoded_dataset = dataset.map(\n",
    "        _preprocess_batch, \n",
    "        batched=True, \n",
    "        remove_columns=[\"text\", \"query\", \"response\"]\n",
    "    )\n",
    "    \n",
    "    split_dataset = encoded_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"customer-support-lora\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,     \n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None,                   \n",
    "        remove_unused_columns=False,      \n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=False,\n",
    "        bf16=False                        \n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model_for_training,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=split_dataset['train'],\n",
    "        eval_dataset=split_dataset['test'],\n",
    "        data_collator=DataCollatorForSeq2Seq(\n",
    "            tokenizer=tokenizer, \n",
    "            model=model_for_training,\n",
    "            max_length=512, \n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    model_for_training.config.use_cache = False\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"customer-support-lora\")\n",
    "    \n",
    "    return trainer, model_for_training\n",
    "\n",
    "trainer, trained_model = complete_lora_training_from_raw_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cf5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''try this on powerful macs or on the cloud'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
